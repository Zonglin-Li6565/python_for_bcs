Homework 12's questions involve the week's readings, and running the lab programs for week 12.
Make sure you have downloaded the most up to date versions that we used in class Thursday's class.

1. Tuesday's lecture and the assigned reading described three main branches of machine learning. What are they, and what
are the situations in which each are useful?

  * Supervised learning: If the training data you have are labeled, and you want
                         to classify new data
  * Unuspervised learning: If you want to find patterns in data, such as
                           clusters
  * Reinforcement learning: If you are dealing with some environment, and action
                            will change state of the environment

2. If we were going to put it into one of the categories, what type of machine learning would a t-test fall into?
Explain your answer?

  It should be supervised learning. The task here is to classify whether the
  distribution is the same as the target distribution, and we want to find the
  best p value to have low false positive and false negative

3. What is over-fitting? How does it relate to the idea of type 1 and type 2 errors from statistics?

  Overfitting means your model overly explored the features and will have bad
  performance when showed with different dataset with different feature
  distribution.
  I believe type 1 error relates better with over-fitting, as it's overly
  exploring features can sometimes lead to false positives.

4. In lab we have been using the k-nearest-neighbor algorithm. Which of the three categories is this algorithm in?
Why does it belong in that category? How does the algorithm work?

  * KNN belongs to supervised learning, since it's used for classification tasks
  * The algorithm works by finding N nearest data points, and take the label of
    the majority as the one for the new data point.

5. What are some situations where a k-nearest neighbor algorithm might work well? What are some situations where it
wouldn't work very well?

  It depends on the distance function. Assume we are using euclidean distance,
  then if the data doesn't have a lot of data points and dimension is not high,
  then KNN might work well. But if either we have a lot of data and each data
  point has a lot of dimensions, then KNN will not work well, as it has to
  store all the train data in order to do the inference. Also, in some situation
  we have features on different scale, for example, one is on 1000 scale, and
  another is on 0.0001, then the larger one will dominate, and KNN wouldn't
  perform well.

6. In 'classify.py', what does changing the MIN_MAX_KNN parameter do? How does this effect the way the algorithm
performs on data1.csv? Are there values that "break" the program and cause it to crash? Why?

  It controls how many different KNNs we will try, and how many neighbors in
  each KNN. The more neighbors you have, the more smooth the decision
  boundary is. If we set the max value to be larger than the number of data we
  have, the program will fail.

7. What effect does the TRAINING_PROPORTION parameter have, both in terms of how it changes what the
program does, and also in terms of how well the k-nearest-neighbor algorithm is able to classify the data in data1.csv?

  It dictates how much we split the data set for validation (test). If the
  number is too large, then nothing will left for validation (test) set.
  if the number is too small, then we won't have enough data for training, and
  will result in underfitting.

8. Why is the k-nearest-neighbor algorithm's accuracy not always the same? Hint: there is a random component. But what
is being randomized? What one line of code could we add to 'classify.py' to make the results consistent and replicable?
What would we be sacrificing by doing so?

  The data will be shuffled every time, so the data points used for training
  will be different. We can set the seed to a constant in classify.py:
  `random.seed(12345)`

9. Why does the k-nearest-neighbor algorithm perform better on data2.csv than on data1.csv? Explain this in terms of the
details of how the algorithm works, not just in terms of why the data in data2.csv is better.

  data2.csv has more (meaningful) features. This will lead to larger distance
  if two data points are not from the same class, and thus will be more
  distinguishable.

10. What does class method Dataset.compute_feature_correlations() do? What do you learn about the data, and the
performance of the classification algorithm, from the output of this method? Make sure you have VERBOSE=True.

  It computes a correlation matrix of each feature, as well as correlation of
  each feature with the class. We can see that some features have strong
  correlation with the class such as wing proportion, has feathers etc.
  When comparing data1.csv with data2.csv, we can see the features in data1.csv
  generally don't have strong correlation with the class, which explains why
  performance on data2.csv is better.

11. If you added 20 random features to data2.csv, but retained the existing features, would that make the
k-nearest-neighbor algorithm perform better, worse, or have no effect? Why?

  It depends on the magnitude of the random noise. If the random noise is small
  then it will help preventing overfitting. But if the noise has large
  magnitude, then it will mess up the relationship between each data point, and
  harm the performance.